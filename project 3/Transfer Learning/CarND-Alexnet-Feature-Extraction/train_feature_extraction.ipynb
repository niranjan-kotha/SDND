{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "import timeit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from alexnet import AlexNet\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "\n",
    "# TODO: Load traffic signs data.\n",
    "import pickle\n",
    "\n",
    "\n",
    "# TODO: fill this in based on where you saved the training and testing data\n",
    "training_file ='train.p'\n",
    "#testing_file ='test.p'\n",
    "\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "#with open(testing_file, mode='rb') as f:\n",
    " #   test = pickle.load(f)\n",
    "n_classes = 43\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "def one_hot_encode(y, num_examples, num_classes):\n",
    "    ret_y = np.zeros((num_examples, num_classes))\n",
    "    for r in range(num_examples):\n",
    "        ret_y[r][y[r]] = 1\n",
    "    return ret_y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['labels', 'features', 'sizes', 'coords'])\n",
      "(29406, 43)\n",
      "(?, 43)\n",
      "(?, 43)\n",
      "(?, 43)\n"
     ]
    }
   ],
   "source": [
    "print(train.keys())\n",
    "# TODO: Split data into training and validation sets.\n",
    "X_train, X_val, y_train, y_val = train_test_split( train['features'], train['labels'], test_size=0.25, random_state=42)\n",
    "y_train=one_hot_encode(y_train,len(y_train),n_classes)\n",
    "y_val=one_hot_encode(y_val,len(y_val),n_classes)\n",
    "\n",
    "#X_test=test['features']\n",
    "#y_test=test['labels']\n",
    "print(y_train.shape)\n",
    "# TODO: Define placeholders and resize operation.\n",
    "\n",
    "features = tf.placeholder(\"float\", [None, 32, 32,3])\n",
    "labels = tf.placeholder(\"float\", [None, n_classes])\n",
    "print(labels.get_shape())\n",
    "resized_features = tf.image.resize_images(features, (227, 227))\n",
    "\n",
    "\n",
    "# TODO: pass placeholder as first argument to `AlexNet`.\n",
    "fc7 = AlexNet(resized_features, feature_extract=True)\n",
    "# NOTE: `tf.stop_gradient` prevents the gradient from flowing backwards\n",
    "# past this point, keeping the weights before and up to `fc7` frozen.\n",
    "# This also makes training faster, less work to do!\n",
    "fc7 = tf.stop_gradient(fc7)\n",
    "\n",
    "\n",
    "shape = (fc7.get_shape().as_list()[-1], n_classes)\n",
    "fc8W = tf.Variable(tf.truncated_normal(shape, stddev=1e-2))\n",
    "fc8b = tf.Variable(tf.zeros(n_classes))\n",
    "logits = tf.nn.xw_plus_b(fc7, fc8W, fc8b)\n",
    "probs = tf.nn.softmax(logits)\n",
    "\n",
    "# TODO: Define loss, training, accuracy operations.\n",
    "# HINT: Look back at your traffic signs project solution, you may\n",
    "# be able to reuse some the code.\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, labels))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(cost)\n",
    "\n",
    "# TODO: Train and evaluate the feature extraction model.\n",
    "learning_rate = 1e-3\n",
    "training_epochs=8\n",
    "\n",
    "print(logits.get_shape())\n",
    "print(labels.get_shape())\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)                                                                                                                              #print(training_epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29406\n",
      "0\n",
      "128\n",
      "256\n",
      "384\n",
      "512\n",
      "640\n",
      "768\n",
      "896\n",
      "1024\n",
      "1152\n",
      "1280\n",
      "1408\n",
      "1536\n",
      "1664\n",
      "1792\n",
      "1920\n",
      "2048\n",
      "2176\n",
      "2304\n",
      "2432\n",
      "2560\n",
      "2688\n",
      "2816\n",
      "2944\n",
      "3072\n",
      "3200\n",
      "3328\n",
      "3456\n",
      "3584\n",
      "3712\n",
      "3840\n",
      "3968\n",
      "4096\n",
      "4224\n",
      "4352\n",
      "4480\n",
      "4608\n",
      "4736\n",
      "4864\n",
      "4992\n",
      "5120\n",
      "5248\n",
      "5376\n",
      "5504\n",
      "5632\n",
      "5760\n",
      "5888\n",
      "6016\n",
      "6144\n",
      "6272\n",
      "6400\n",
      "6528\n",
      "6656\n",
      "6784\n",
      "6912\n",
      "7040\n",
      "7168\n",
      "7296\n",
      "7424\n",
      "7552\n",
      "7680\n",
      "7808\n",
      "7936\n",
      "8064\n",
      "8192\n",
      "8320\n",
      "8448\n",
      "8576\n",
      "8704\n",
      "8832\n",
      "8960\n",
      "9088\n",
      "9216\n",
      "9344\n",
      "9472\n",
      "9600\n",
      "9728\n",
      "9856\n",
      "9984\n",
      "10112\n",
      "10240\n",
      "10368\n",
      "10496\n",
      "10624\n",
      "10752\n",
      "10880\n",
      "11008\n",
      "11136\n",
      "11264\n",
      "11392\n",
      "11520\n",
      "11648\n",
      "11776\n",
      "11904\n",
      "12032\n",
      "12160\n",
      "12288\n",
      "12416\n",
      "12544\n",
      "12672\n",
      "12800\n",
      "12928\n",
      "13056\n",
      "13184\n",
      "13312\n",
      "13440\n",
      "13568\n",
      "13696\n",
      "13824\n",
      "13952\n",
      "14080\n",
      "14208\n",
      "14336\n",
      "14464\n",
      "14592\n",
      "14720\n",
      "14848\n",
      "14976\n",
      "15104\n",
      "15232\n",
      "15360\n",
      "15488\n",
      "15616\n",
      "15744\n",
      "15872\n",
      "16000\n",
      "16128\n",
      "16256\n",
      "16384\n",
      "16512\n",
      "16640\n",
      "16768\n",
      "16896\n",
      "17024\n",
      "17152\n",
      "17280\n",
      "17408\n",
      "17536\n",
      "17664\n",
      "17792\n",
      "17920\n",
      "18048\n",
      "18176\n",
      "18304\n",
      "18432\n",
      "18560\n",
      "18688\n",
      "18816\n",
      "18944\n",
      "19072\n",
      "19200\n",
      "19328\n",
      "19456\n",
      "19584\n",
      "19712\n",
      "19840\n",
      "19968\n",
      "20096\n",
      "20224\n",
      "20352\n",
      "20480\n",
      "20608\n",
      "20736\n",
      "20864\n",
      "20992\n",
      "21120\n",
      "21248\n",
      "21376\n",
      "21504\n",
      "21632\n",
      "21760\n",
      "21888\n",
      "22016\n",
      "22144\n",
      "22272\n",
      "22400\n",
      "22528\n",
      "22656\n",
      "22784\n",
      "22912\n",
      "23040\n",
      "23168\n",
      "23296\n",
      "23424\n",
      "23552\n",
      "23680\n",
      "23808\n",
      "23936\n",
      "24064\n",
      "24192\n",
      "24320\n",
      "24448\n",
      "24576\n",
      "24704\n",
      "24832\n",
      "24960\n",
      "25088\n",
      "25216\n",
      "25344\n",
      "25472\n",
      "25600\n",
      "25728\n",
      "25856\n",
      "25984\n",
      "26112\n",
      "26240\n",
      "26368\n",
      "26496\n",
      "26624\n",
      "26752\n",
      "26880\n",
      "27008\n",
      "27136\n",
      "27264\n",
      "27392\n",
      "27520\n",
      "27648\n",
      "27776\n",
      "27904\n",
      "28032\n",
      "28160\n",
      "28288\n",
      "28416\n",
      "28544\n",
      "28672\n",
      "28800\n",
      "28928\n",
      "29056\n",
      "29184\n",
      "29312\n",
      "Epoch: 0001 cost= 0.197187722\n",
      "time in seconds 830.9714002700002\n",
      "Optimization Finished!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b8bee27db944>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time in seconds'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Optimization Finished!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overall training Accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_training_data\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m                                                                                                                        \u001b[0;31m#print(\"overall training Accuracy:\",accuracy2.eval({x: Data.training_data, y: Data.labels_training_data}))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overall training Accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_testing_data\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m                                                                                                                             \u001b[0;31m#print(\"overall testing Accuracy:\", accuracy2.eval({x: Data.testing_data, y: Data.labels_testing_data}))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "training_epochs=1\n",
    "print(X_train.shape[0])\n",
    "for epoch in range(training_epochs):        \n",
    "    t0 = timeit.default_timer()        \n",
    "    X_train, y_train = shuffle(X_train, y_train)    \n",
    "    for offset in range(0, X_train.shape[0], batch_size):\n",
    "        print(offset)\n",
    "        end = offset + batch_size        \n",
    "        sess.run(optimizer, feed_dict={features: X_train[offset:end], labels: y_train[offset:end]})\n",
    "    c = sess.run(cost, feed_dict={features: X_train[offset:end], labels: y_train[offset:end]})\n",
    "    print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c))\n",
    "    print('time in seconds',timeit.default_timer()-t0)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"overall training Accuracy:\",sess.run(accuracy,{x: Data.training_data, y: Data.labels_training_data}))                                                                                                                        #print(\"overall training Accuracy:\",accuracy2.eval({x: Data.training_data, y: Data.labels_training_data}))\n",
    "print(\"overall training Accuracy:\",sess.run(accuracy,{x: Data.testing_data, y: Data.labels_testing_data}))                                                                                                                             #print(\"overall testing Accuracy:\", accuracy2.eval({x: Data.testing_data, y: Data.labels_testing_data}))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from alexnet import AlexNet\n",
    "\n",
    "nb_classes = 43\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "with open('./train.p', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(data['features'], data['labels'], test_size=0.33, random_state=0)\n",
    "\n",
    "features = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "labels = tf.placeholder(tf.int64, None)\n",
    "resized = tf.image.resize_images(features, (227, 227))\n",
    "\n",
    "# Returns the second final layer of the AlexNet model,\n",
    "# this allows us to redo the last layer for the traffic signs\n",
    "# model.\n",
    "fc7 = AlexNet(resized, feature_extract=True)\n",
    "fc7 = tf.stop_gradient(fc7)\n",
    "shape = (fc7.get_shape().as_list()[-1], nb_classes)\n",
    "fc8W = tf.Variable(tf.truncated_normal(shape, stddev=1e-2))\n",
    "fc8b = tf.Variable(tf.zeros(nb_classes))\n",
    "logits = tf.nn.xw_plus_b(fc7, fc8W, fc8b)\n",
    "\n",
    "\n",
    "\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels)\n",
    "loss_op = tf.reduce_mean(cross_entropy)\n",
    "opt = tf.train.AdamOptimizer()\n",
    "train_op = opt.minimize(loss_op, var_list=[fc8W, fc8b])\n",
    "init_op = tf.initialize_all_variables()\n",
    "\n",
    "preds = tf.arg_max(logits, 1)\n",
    "accuracy_op = tf.reduce_mean(tf.cast(tf.equal(preds, labels), tf.float32))\n",
    "\n",
    "\n",
    "def eval_on_data(X, y, sess):\n",
    "    total_acc = 0\n",
    "    total_loss = 0\n",
    "    for offset in range(0, X.shape[0], batch_size):\n",
    "        end = offset + batch_size\n",
    "        X_batch = X[offset:end]\n",
    "        y_batch = y[offset:end]\n",
    "\n",
    "        loss, acc = sess.run([loss_op, accuracy_op], feed_dict={features: X_batch, labels: y_batch})\n",
    "        total_loss += (loss * X_batch.shape[0])\n",
    "        total_acc += (acc * X_batch.shape[0])\n",
    "\n",
    "    return total_loss/X.shape[0], total_acc/X.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Time: 114.877 seconds\n",
      "Validation Loss = 0.517980763607\n",
      "Validation Accuracy = 0.865136409305\n",
      "\n",
      "Epoch 2\n",
      "Time: 109.237 seconds\n",
      "Validation Loss = 0.343903157263\n",
      "Validation Accuracy = 0.909498415689\n",
      "\n",
      "Epoch 3\n",
      "Time: 109.271 seconds\n",
      "Validation Loss = 0.262174333226\n",
      "Validation Accuracy = 0.937707705433\n",
      "\n",
      "Epoch 4\n",
      "Time: 109.251 seconds\n",
      "Validation Loss = 0.214463703684\n",
      "Validation Accuracy = 0.947600278252\n",
      "\n",
      "Epoch 5\n",
      "Time: 109.247 seconds\n",
      "Validation Loss = 0.192281071455\n",
      "Validation Accuracy = 0.951309993044\n",
      "\n",
      "Epoch 6\n",
      "Time: 109.249 seconds\n",
      "Validation Loss = 0.167960695862\n",
      "Validation Accuracy = 0.957106422467\n",
      "\n",
      "Epoch 7\n",
      "Time: 109.240 seconds\n",
      "Validation Loss = 0.161128025032\n",
      "Validation Accuracy = 0.956410850916\n",
      "\n",
      "Epoch 8\n",
      "Time: 109.242 seconds\n",
      "Validation Loss = 0.146936610655\n",
      "Validation Accuracy = 0.962361851766\n",
      "\n",
      "Epoch 9\n",
      "Time: 109.256 seconds\n",
      "Validation Loss = 0.140570339986\n",
      "Validation Accuracy = 0.961820851712\n",
      "\n",
      "Epoch 10\n",
      "Time: 109.264 seconds\n",
      "Validation Loss = 0.123753133462\n",
      "Validation Accuracy = 0.966071566582\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        # training\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        t0 = time.time()\n",
    "        for offset in range(0, X_train.shape[0], batch_size):\n",
    "            end = offset + batch_size\n",
    "            sess.run(train_op, feed_dict={features: X_train[offset:end], labels: y_train[offset:end]})\n",
    "\n",
    "        val_loss, val_acc = eval_on_data(X_val, y_val, sess)\n",
    "        print(\"Epoch\", i+1)\n",
    "        print(\"Time: %.3f seconds\" % (time.time() - t0))\n",
    "        print(\"Validation Loss =\", val_loss)\n",
    "        print(\"Validation Accuracy =\", val_acc)\n",
    "        print(\"\")\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:keras]",
   "language": "python",
   "name": "conda-env-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
